tokenize: Tokenize
vocal: Tokenize/vocab.json
model_pretrained: Model_pretrained
dataset_train: dataset_sample/data_train.json
dataset_test: dataset_sample/data_test.json
num_proc: 4
max_input_length_in_sec: 20
batch: 1
output_dir: checkpoint
num_epoch: 100
save_total_limit: 3
save_steps: 100
logging_steps: 500
learning_rate: 0.0001
weight_decay: 0.005
warmup_steps: 1000
fp16: True
eval_steps: 500
wav_test: t1_utt000000042.wav
